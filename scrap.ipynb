{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fe67d5",
   "metadata": {},
   "source": [
    "# Medium Scraping\n",
    "\n",
    "This notebook teach you how to collect articles data from Medium, filtering with tags and release date, and put it into a csv file.\n",
    "\n",
    "It was constructed based on [Dorian Lazar](https://dorianlazar.medium.com/) article that can be found \n",
    "[Here](https://dorianlazar.medium.com/scraping-medium-with-python-beautiful-soup-3314f898bbf5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62549887",
   "metadata": {},
   "source": [
    "## Scraping stages\n",
    "\n",
    "1. Create Filters\n",
    "2. Get all articles with selected tags and release date\n",
    "3. Iterates articles getting usefull infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tools\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b296d",
   "metadata": {},
   "source": [
    "## Create filters\n",
    "\n",
    "We need two three filtes.\n",
    "\n",
    "### tag_urls\n",
    "A dictionary named 'urls', where the key is the tag name and the values reference the url that access the Medium archives of an specific tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089b0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_urls = {\n",
    "    'Data Science': 'https://medium.com/tag/data-science/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Machine Learning': 'https://medium.com/tag/machine-learning/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Artificial Inteligence': 'https://medium.com/tag/artificial-intelligence/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Deep Learning': 'https://medium.com/tag/deep-learning/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Data': 'https://medium.com/tag/data/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Big Data': 'https://medium.com/tag/big-data/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Analytics': 'https://medium.com/tag/analytics/archive/{0}/{1:02d}/{2:02d}',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d519f0",
   "metadata": {},
   "source": [
    "### year\n",
    "\n",
    "A integer that represents our articles release year.\n",
    "\n",
    "### selected_days\n",
    "\n",
    "This one is a bit more trick, it is a list on integers that represents the day of the year in sequential order.\n",
    "\n",
    "Where 1 represents january 1, and 366 represents December 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161a351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "selected_days = [i for i in range(1, 366)] #All days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9ed72",
   "metadata": {},
   "source": [
    "## Create support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b19d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_day(day):\n",
    "    # if it is a leap year use month_days = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    m = 0\n",
    "    d = 0\n",
    "    while day > 0:\n",
    "        d = day\n",
    "        day -= month_days[m]\n",
    "        m += 1\n",
    "    return (m, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0c774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claps(claps_str):\n",
    "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
    "        return 0\n",
    "    split = claps_str.split('K')\n",
    "    claps = float(split[0])\n",
    "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
    "    return claps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75921003",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "\n",
    "Collect Medium data, and put it into a list called articles_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a38c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n",
      "2020-01-02\n",
      "2020-01-03\n",
      "2020-01-04\n",
      "2020-01-05\n",
      "2020-01-06\n",
      "2020-01-07\n",
      "2020-01-08\n",
      "2020-01-09\n",
      "2020-01-10\n",
      "2020-01-11\n",
      "2020-01-12\n",
      "2020-01-13\n",
      "2020-01-14\n",
      "2020-01-15\n",
      "2020-01-16\n",
      "2020-01-17\n",
      "2020-01-18\n",
      "2020-01-19\n",
      "2020-01-20\n",
      "2020-01-21\n",
      "2020-01-22\n",
      "2020-01-23\n",
      "2020-01-24\n",
      "2020-01-25\n",
      "2020-01-26\n",
      "2020-01-27\n",
      "2020-01-28\n",
      "2020-01-29\n",
      "2020-01-30\n",
      "2020-01-31\n",
      "2020-02-01\n",
      "2020-02-02\n",
      "2020-02-03\n",
      "2020-02-04\n",
      "2020-02-05\n",
      "2020-02-06\n",
      "2020-02-07\n",
      "2020-02-08\n",
      "2020-02-09\n",
      "2020-02-10\n",
      "2020-02-11\n",
      "2020-02-12\n",
      "2020-02-13\n",
      "2020-02-14\n",
      "2020-02-15\n",
      "2020-02-16\n",
      "2020-02-17\n",
      "2020-02-18\n",
      "2020-02-19\n",
      "2020-02-20\n",
      "2020-02-21\n",
      "2020-02-22\n",
      "2020-02-23\n",
      "2020-02-24\n",
      "2020-02-25\n",
      "2020-02-26\n",
      "2020-02-27\n",
      "2020-02-28\n",
      "2020-03-01\n",
      "2020-03-02\n",
      "2020-03-03\n",
      "2020-03-04\n",
      "2020-03-05\n"
     ]
    }
   ],
   "source": [
    "articles_data = []\n",
    "article_id = 0\n",
    "n = len(selected_days)\n",
    "for d in selected_days:\n",
    "    month, day = convert_day(d)\n",
    "    date = '{0}-{1:02d}-{2:02d}'.format(year, month, day) \n",
    "    print(f'{date}')\n",
    "    for tag, url in tag_urls.items(): \n",
    "        response = requests.get(url.format(year, month, day), allow_redirects=True)\n",
    "        if not response.url.startswith(url.format(year, month, day)):\n",
    "            continue\n",
    "        page = response.content\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        articles = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
    "        \n",
    "        for article in articles:\n",
    "            \n",
    "            title = article.find(\"h3\", class_=\"graf--title\")\n",
    "            if title is None:\n",
    "                continue\n",
    "            title = title.contents[0]\n",
    "            \n",
    "            author = article.find_all(\"a\")[0]['href'].split('?')[0].split('@')[1]\n",
    "            author_url = article.find_all(\"a\")[0]['href'].split('?')[0]\n",
    "            \n",
    "            subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
    "            subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
    "            \n",
    "            article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
    "            \n",
    "            claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
    "            \n",
    "            reading_time = article.find(\"span\", class_=\"readingTime\")\n",
    "            reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
    "            \n",
    "            responses = article.find_all(\"a\")\n",
    "            if len(responses) == 7:\n",
    "                responses = responses[6].contents[0].split(' ')\n",
    "                if len(responses) == 0:\n",
    "                    responses = 0\n",
    "                else:\n",
    "                    responses = responses[0]\n",
    "            else:\n",
    "                responses = 0\n",
    "\n",
    "            articles_data.append([article_url, title,\n",
    "                         author, author_url,\n",
    "                         subtitle, claps, responses,\n",
    "                         reading_time, tag, date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37c8fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform article data into panda dataframe.\n",
    "medium_df = pd.DataFrame(articles_data, columns=[\n",
    "    'url', 'title', 'author', 'author_page',\n",
    "    'subtitle', 'claps', 'responses', 'reading_time',\n",
    "    'tag', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc2173",
   "metadata": {},
   "source": [
    "## Remove duplicated data\n",
    "\n",
    "As we can search about similar tags, it can bring the same articles in different iteration, so we need to clean our collected data.\n",
    "\n",
    "We do it using the panda fucntion drop_duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8859b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c12b9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_df = medium_df.drop_duplicates(subset=['url', 'title'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343be66",
   "metadata": {},
   "source": [
    "## The final data\n",
    "\n",
    "Lets take a look of how our collected data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66548f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'medium_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28755/2055672375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmedium_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'medium_df' is not defined"
     ]
    }
   ],
   "source": [
    "medium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc1aa138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_page</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9</td>\n",
       "      <td>180 Data Science and Machine Learning Projects with Python</td>\n",
       "      <td>amankharwal</td>\n",
       "      <td>https://medium.com/@amankharwal</td>\n",
       "      <td>180 Data Science and Machine Learning…</td>\n",
       "      <td>1100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a</td>\n",
       "      <td>Natural Language Generation Part 2: GPT2 and Huggingface</td>\n",
       "      <td>georgedittmar</td>\n",
       "      <td>https://towardsdatascience.com/@georgedittmar</td>\n",
       "      <td>Learn to use Huggingface and GPT-2 to train a…</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Deep Learning</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://medium.com/@scribblr42/working-with-dataviz-in-2021-whats-next-and-what-matters-356db3aa0098</td>\n",
       "      <td>[Working with #DataViz in 2021 — What’s Next? (and what matters?)]</td>\n",
       "      <td>scribblr42</td>\n",
       "      <td>https://medium.com/@scribblr42</td>\n",
       "      <td></td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Data</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://medium.com/@dharmeshpanchmatia/data-analytics-and-ai-ml-platform-for-ecommerce-68639df89c7f</td>\n",
       "      <td>Data Analytics and AI/ML platform for eCommerce</td>\n",
       "      <td>dharmeshpanchmatia</td>\n",
       "      <td>https://medium.com/@dharmeshpanchmatia</td>\n",
       "      <td>Improve user pr</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79</td>\n",
       "      <td>Understanding the Confusion Matrix from Scikit learn</td>\n",
       "      <td>samarthagrawal86</td>\n",
       "      <td>https://towardsdatascience.com/@samarthagrawal86</td>\n",
       "      <td>Clear representation of output of confusion…</td>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Analytics</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   1   \n",
       "3   4   \n",
       "4   5   \n",
       "5   6   \n",
       "6   7   \n",
       "\n",
       "                                                                                                      url  \\\n",
       "0  https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9   \n",
       "3    https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a   \n",
       "4    https://medium.com/@scribblr42/working-with-dataviz-in-2021-whats-next-and-what-matters-356db3aa0098   \n",
       "5     https://medium.com/@dharmeshpanchmatia/data-analytics-and-ai-ml-platform-for-ecommerce-68639df89c7f   \n",
       "6        https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79   \n",
       "\n",
       "                                                                title  \\\n",
       "0          180 Data Science and Machine Learning Projects with Python   \n",
       "3            Natural Language Generation Part 2: GPT2 and Huggingface   \n",
       "4  [Working with #DataViz in 2021 — What’s Next? (and what matters?)]   \n",
       "5                     Data Analytics and AI/ML platform for eCommerce   \n",
       "6                Understanding the Confusion Matrix from Scikit learn   \n",
       "\n",
       "               author                                       author_page  \\\n",
       "0         amankharwal                   https://medium.com/@amankharwal   \n",
       "3       georgedittmar     https://towardsdatascience.com/@georgedittmar   \n",
       "4          scribblr42                    https://medium.com/@scribblr42   \n",
       "5  dharmeshpanchmatia            https://medium.com/@dharmeshpanchmatia   \n",
       "6    samarthagrawal86  https://towardsdatascience.com/@samarthagrawal86   \n",
       "\n",
       "                                         subtitle  claps responses  \\\n",
       "0          180 Data Science and Machine Learning…   1100         4   \n",
       "3  Learn to use Huggingface and GPT-2 to train a…     39         0   \n",
       "4                                                    202         0   \n",
       "5                                 Improve user pr     30         0   \n",
       "6    Clear representation of output of confusion…    190         5   \n",
       "\n",
       "   reading_time    publication        date  \n",
       "0             4   Data Science  2021-01-01  \n",
       "3             8  Deep Learning  2021-01-01  \n",
       "4            11           Data  2021-01-01  \n",
       "5             5       Big Data  2021-01-01  \n",
       "6             5      Analytics  2021-01-01  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956a296",
   "metadata": {},
   "source": [
    "## Save Collected data into csv file\n",
    "\n",
    "We save our data frame into a csv file named medium_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adb0ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_df.to_csv('medium_data.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84846461",
   "metadata": {},
   "source": [
    "Hope you enjoy this notebook, feel free to give sugestion or submit PRs.\n",
    "\n",
    "Made with love by @viniciusLambert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
